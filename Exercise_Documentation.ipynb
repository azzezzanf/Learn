{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqerZS5FB+TLG7R2984gaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzezzanf/Learn/blob/main/Exercise_Documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise Documentation Part 1\n",
        "Azzezza NF"
      ],
      "metadata": {
        "id": "LZ2rn_7e-XhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Coding Exercise 3: Encoding Categorical Data for Machine Learning\n"
      ],
      "metadata": {
        "id": "rB3yyu-488J7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import required libraries - Pandas, Numpy, and required classes for this task - ColumnTransformer, OneHotEncoder, LabelEncoder.\n",
        "\n",
        "2. Start by loading the Titanic dataset into a pandas data frame. This can be done using the pd.read_csv function. The dataset's name is 'titanic.csv'.\n",
        "\n",
        "3. Identify the categorical features in your dataset that need to be encoded. You can store these feature names in a list for easy access later.\n",
        "\n",
        "4. To apply OneHotEncoding to these categorical features, create an instance of the ColumnTransformer class. Make sure to pass the OneHotEncoder() as an argument along with the list of categorical features.\n",
        "\n",
        "5. Use the fit_transform method on the instance of ColumnTransformer to apply the OneHotEncoding.\n",
        "\n",
        "6. The output of the fit_transform method should be converted into a NumPy array for further use.\n",
        "\n",
        "7. The 'Survived' column in your dataset is the dependent variable. This is a binary categorical variable that should be encoded using LabelEncoder.\n",
        "\n",
        "8.  Print the updated matrix of features and the dependent variable **vector**"
      ],
      "metadata": {
        "id": "9iWcjfxo-VfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AzzezzaNF"
      ],
      "metadata": {
        "id": "mWJ8mKsp-FVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01wLxSr583fz"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Identify the categorical data\n",
        "categorical_features = ['Embarked', 'Pclass', 'Sex']\n",
        "\n",
        "# Implement an instance of the ColumnTransformer class\n",
        "ct = ColumnTransformer(transformers =[('encoder', OneHotEncoder(), categorical_features)] , remainder = 'passthrough' )\n",
        "\n",
        "# Apply the fit_transform method on the instance of ColumnTransformer\n",
        "X = ct.fit_transform(df)\n",
        "\n",
        "# Convert the output into a NumPy array\n",
        "X = np.array(X)\n",
        "\n",
        "# Use LabelEncoder to encode binary categorical data\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['Survived'])\n",
        "\n",
        "# Print the updated matrix of features and the dependent variable vector\n",
        "print(X)\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explanation\n",
        "\n",
        "1. Importing the necessary libraries. Import pandas for data manipulation, numpy for numerical operations, and the necessary classes from scikit-learn for preprocessing.\n",
        "```\n",
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "```\n",
        "\n",
        "2. Load the dataset. The Titanic dataset is loaded into a pandas DataFrame from a CSV file.\n",
        "\n",
        "```\n",
        "# Load the dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "```\n",
        "\n",
        "3. Identify the categorical data. Specify which features in our dataset are categorical. In this case, 'Sex', 'Embarked', and 'Pclass' are the categorical features.\n",
        "\n",
        "```\n",
        "# Identify the categorical data\n",
        "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
        "```\n",
        "\n",
        "4. Implement an instance of the ColumnTransformer clas. Initialize a ColumnTransformer that will apply a OneHotEncoder to the categorical features. The remainder='passthrough' argument ensures that the non-transformed features are not discarded.\n",
        "\n",
        "```\n",
        "# Implement an instance of the ColumnTransformer class\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('encoder', OneHotEncoder(), categorical_features)\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "```\n",
        "5. Apply the fit_transform method. Fit the ColumnTransformer to our DataFrame and transform the data. This applies one-hot encoding to our categorical features, converting them into numerical data suitable for a machine-learning model.\n",
        "\n",
        "```\n",
        "# Apply the fit_transform method on the instance of ColumnTransformer\n",
        "X = ct.fit_transform(df)\n",
        "```\n",
        "6. Convert the output into a NumPy array. Convert the output to a NumPy array: The output of the ColumnTransformer is a sparse matrix - convert this to a dense NumPy array for easier manipulation.\n",
        "\n",
        "```\n",
        "# Convert the output into a NumPy array\n",
        "X = np.array(X)\n",
        "```\n",
        "\n",
        "7. Use LabelEncoder to encode binary categorical data. The 'Survived' feature is our dependent variable. Since it is a binary categorical feature, we use LabelEncoder to transform it into numerical data.\n",
        "\n",
        "```\n",
        "# Use LabelEncoder to encode binary categorical data\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['Survived'])\n",
        "```\n",
        "\n",
        "8. Print the transformed feature matrix and dependent variable vector to verify that our preprocessing steps have been applied correctly.\n",
        "\n",
        "```\n",
        "# Print the updated matrix of features and the dependent variable vector\n",
        "print(\"Updated matrix of features: \\n\", X)\n",
        "print(\"Updated dependent variable vector: \\n\", y)\n",
        "```"
      ],
      "metadata": {
        "id": "Mzs0a3qw9QuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Exercise 4: Dataset Splitting and Feature Scaling"
      ],
      "metadata": {
        "id": "WdIt5QVLKcRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coding Exercise 4: Dataset Splitting and Feature Scaling\n",
        "\n",
        "1. Import necessary Python libraries: pandas, train_test_split from sklearn.model_selection, and StandardScaler from sklearn.preprocessing.\n",
        "\n",
        "2. Load the Iris dataset using Pandas read.csv. Dataset name is iris.csv.\n",
        "\n",
        "3. Use train_test_split to split the dataset into an 80-20 training-test set.\n",
        "\n",
        "4. Apply random_state with 42 value in train_test_split function for reproducible results.\n",
        "\n",
        "5. Print X_train, X_test, Y_train, and Y_test to understand the dataset split.\n",
        "\n",
        "6. Use StandardScaler to apply feature scaling on the training and test sets.\n",
        "\n",
        "7. Print scaled training and test sets to verify feature scaling."
      ],
      "metadata": {
        "id": "Y-yY9yoKn2Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "df = pd.read_csv('iris.csv')\n",
        "# Separate features and target\n",
        "X = df.drop(columns='target', axis = 1)\n",
        "y = df['target']\n",
        "# Split the dataset into an 80-20 training-test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state =42)\n",
        "print('x train : \\n', x_train)\n",
        "print('\\n x test : \\n', x_test)\n",
        "print('\\n y train : \\n', y_train)\n",
        "print('\\n x test : \\n', y_test)\n",
        "# Apply feature scaling on the training and test sets\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(x_train)\n",
        "X_test = sc.fit_transform(x_test)\n",
        "# Print the scaled training and test sets\n",
        "print('x train scaled : \\n', X_train)\n",
        "print('\\n x test scaled : \\n', X_test)"
      ],
      "metadata": {
        "id": "27YbSsILKfDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding exercise 5: Feature scaling for Machine Learning"
      ],
      "metadata": {
        "id": "79hMvAOE3u5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import the necessary libraries for data preprocessing, including the\n",
        "\n",
        "2. StandardScaler and train_test_split classes.\n",
        "\n",
        "3. Load the \"Wine Quality Red\" dataset into a pandas DataFrame. You can use the pd.read_csv function for this. Make sure you set the correct delimeter for the file.\n",
        "\n",
        "4. Split your dataset into an 80-20 training-test set. Set random_state to 42 to ensure reproducible results.\n",
        "\n",
        "5. Create an instance of the StandardScaler class.\n",
        "\n",
        "6. Fit the StandardScaler on features from the training set, excluding the target variable 'Quality'.\n",
        "\n",
        "7. Use the \"fit_transform\" method of the StandardScaler object on the training dataset.\n",
        "\n",
        "8. Apply the \"transform\" method of the StandardScaler object on the test dataset.\n",
        "\n",
        "9. Print your scaled training and test datasets to verify the feature scaling process.\n",
        "\n"
      ],
      "metadata": {
        "id": "TUTSm-I331S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Wine Quality Red dataset\n",
        "df = pd.read_csv('winequality-red.csv', delimiter = ';')\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['quality'])\n",
        "y = df['quality']\n",
        "# Split the dataset into an 80-20 training-test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 42)\n",
        "\n",
        "# Create an instance of the StandardScaler class\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler on the features from the training set and transform it\n",
        "X_train = sc.fit_transform(X_train)\n",
        "\n",
        "# Apply the transform to the test set\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Print the scaled training and test datasets\n",
        "print(X_train)\n",
        "print(X_test)"
      ],
      "metadata": {
        "id": "UxroL3Rv3y8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "1. Import necessary libraries: We start by importing the necessary libraries for data preprocessing. This includes pandas for data manipulation, train_test_split from sklearn.model_selection to split our dataset into training and test sets, and StandardScaler from sklearn.preprocessing to apply feature scaling.\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "```\n",
        "\n",
        "2. Load the dataset: The Wine Quality Red dataset is loaded into a pandas DataFrame using the pd.read_csv function. Here, we need to specify the correct delimiter, which in this case is a semicolon.\n",
        "\n",
        "```\n",
        "df = pd.read_csv('winequality-red.csv', delimiter=';')\n",
        "\n",
        "```\n",
        "\n",
        "3. Split the dataset into a training set and a test set: We separate the target variable 'quality' from the features and then split the dataset into an 80-20 training-test set using the train_test_split function.\n",
        "\n",
        "```\n",
        "X = df.drop('quality', axis=1) y = df['quality'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "```\n",
        "\n",
        "4. Create an instance of the StandardScaler class: The StandardScaler class is used to standardize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "```\n",
        "sc = StandardScaler()\n",
        "\n",
        "```\n",
        "5. Fit the StandardScaler on the training set and transform the data: The StandardScaler is fitted to the training set and then used to transform both the training and test datasets.\n",
        "\n",
        "```\n",
        "X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "6. Print the scaled datasets: Finally, we print the scaled training and test datasets to verify the feature scaling process.\n",
        "\n",
        "```\n",
        "print(\"Scaled training set:\\n\", X_train) print(\"Scaled test set:\\n\", X_test)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "OvfoInTD6UHK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aOHxkWUw6vng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}